{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import csv \n",
    "import json\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sn\n",
    "import matplotlib.pyplot as plt\n",
    "from pylab import savefig\n",
    "from datetime import datetime\n",
    "import pandas_profiling\n",
    "import sklearn\n",
    "import xgboost as xgb\n",
    "\n",
    "from scipy.stats import uniform\n",
    "from sklearn.feature_selection import SelectKBest\n",
    "from sklearn.feature_selection import chi2\n",
    "from sklearn.linear_model import Ridge\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.svm import SVC \n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import classification_report\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import RandomizedSearchCV\n",
    "\n",
    "\n",
    "def write_log_header(working_df, data_filename, session_log) :\n",
    "    \"\"\"save summary information to results file\n",
    "    \"\"\"\n",
    "\n",
    "    cols_with_nulls = working_df.columns[df.isnull().any()]\n",
    "    with open(session_log, 'w+') as f:   \n",
    "        f.write('\\nsklearn version: ' + sklearn.__version__)\n",
    "        f.write('\\npandas version: ' + pd.__version__)\n",
    "        f.write('\\nnumpy version: ' + np.__version__)\n",
    "        f.write('\\nPandas Profile version: ' + pandas_profiling.__version__)\n",
    "\n",
    "        f.write('\\n\\nData file ' + data_filename + '\\n\\n')\n",
    "        f.write('Shape: ' + str(working_df.shape) + '\\n')\n",
    "        f.write('Columns with null data: \\n')\n",
    "        if len(cols_with_nulls) == 0:\n",
    "            f.write('None\\n')\n",
    "        else:\n",
    "            for col in cols_with_nulls:\n",
    "                f.write(col + '\\n')\n",
    "\n",
    "        cols = sorted(working_df.columns)\n",
    "        f.write('\\nOriginal columns in data file: \\n')\n",
    "        for item in cols:\n",
    "            f.write(item + '\\n')            \n",
    "\n",
    "\n",
    "def update_columns(working_df, col_info_filename, session_log):\n",
    "    \"\"\" update column names to readable versions\n",
    "        data is read as categorical, update type for 4 numeric columns\n",
    "    \"\"\"\n",
    "    #these are numerical columns and are interpreted as such\n",
    "    working_df['MOD10FWK'] = pd.to_numeric(working_df['MOD10FWK'])\n",
    "    working_df['VIG10FWK'] = pd.to_numeric(working_df['VIG10FWK'])\n",
    "    working_df['STRONGFWK'] = pd.to_numeric(working_df['STRONGFWK'])\n",
    "    \n",
    "    #update column names \n",
    "    info_dict = pd.read_csv(col_info_filename, index_col=0, squeeze=True, header=None).to_dict()\n",
    "    working_df.rename(columns=info_dict, inplace=True)\n",
    "\n",
    "    cols = sorted(working_df.columns)\n",
    "    \n",
    "    with open(session_log, 'a') as f: \n",
    "        f.write('\\nrenamed columns in data file: \\n')\n",
    "        for item in cols:\n",
    "            f.write(item + '\\n')  \n",
    "\n",
    "    return working_df\n",
    "\n",
    "\n",
    "def drop_columns(working_df, cols_to_drop_filename, session_log):\n",
    "    \"\"\"drop columns with high percentage of unusable data and/or high correlation\n",
    "       to other non-target features \n",
    "    \"\"\"\n",
    "    \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nShape before dropping columns\\n' + str(working_df.shape))\n",
    "   \n",
    "    #read columns from file\n",
    "    with open(cols_to_drop_filename, 'r') as f_cols:\n",
    "        reader = csv.reader(f_cols)\n",
    "        cols_to_drop = [r[0] for r in reader]\n",
    "    \n",
    "    #drop columns\n",
    "    working_df.drop(columns=cols_to_drop, axis=1, inplace=True)\n",
    "    \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nShape after dropping columns\\n' + str(working_df.shape))\n",
    "    \n",
    "        cols = sorted(working_df.columns)\n",
    "        f.write('\\nFinal columns in data file: \\n')\n",
    "        for item in cols:\n",
    "            f.write(item + '\\n')            \n",
    "\n",
    "    return working_df\n",
    "\n",
    "\n",
    "def drop_invalid_target_rows(working_df, session_log):\n",
    "    \"\"\"update dataset to only use rows with target = no (1) or yes (2)\n",
    "    \"\"\"\n",
    "    \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nShape before deleting invalid target rows\\n' + str(working_df.shape))\n",
    "   \n",
    "    new_df = working_df[(working_df['HYPERTENEV'] !='0') & \\\n",
    "                        (working_df['HYPERTENEV'] !='7') & \\\n",
    "                        (working_df['HYPERTENEV'] !='9') & \\\n",
    "                        (working_df['HYPERTENEV'] !='8') ]\n",
    "   \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nShape after deleting invalid target rows\\n' + str(new_df.shape))\n",
    "        f.write('\\nhypertension value counts:\\n ')\n",
    "        counts = new_df['HYPERTENEV'].value_counts()\n",
    "        f.write('\\nindex: ' + str(counts.index.tolist()))\n",
    "        f.write('\\nvalues: ' + str(counts.values.tolist()))\n",
    "            \n",
    "    return new_df\n",
    "\n",
    "def drop_missing_data_rows(working_df, session_log, after_rename = False):\n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nShape before deleting missing data rows\\n' + str(working_df.shape))\n",
    "    \n",
    "    if after_rename:\n",
    "        new_df = working_df[(working_df['Ever had chickenpox'] !='0') & \\\n",
    "                            (working_df['Age first smoked fairly regularly'] !='0') & \\\n",
    "                            (working_df['Kind of usual place for medical care'] !='0')]\n",
    "        counts = new_df['Ever told had hypertension'].value_counts()\n",
    "    else:\n",
    "        new_df = working_df[(working_df['CPOXEV'] !='0') & \\\n",
    "                            (working_df['SMOKAGEREG'] !='0') & \\\n",
    "                            (working_df['TYPPLSICK'] !='0')]\n",
    "        counts = new_df['HYPERTENEV'].value_counts()\n",
    "    \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nShape after deleting missing data rows\\n' + str(new_df.shape))\n",
    "        f.write('\\nhypertension value counts:\\n ')\n",
    "        print(counts)\n",
    "        f.write('\\nindex: ' + str(counts.index.tolist()))\n",
    "        f.write('\\nvalues: ' + str(counts.values.tolist()))\n",
    "            \n",
    "    return new_df\n",
    "\n",
    "def run_Pandas_Profiling(working_df, session_log, break_into_sections = False, num_cols=5):\n",
    "    \"\"\"Run Pandas_Profiling \n",
    "    \"\"\"\n",
    "    t0 = datetime.now()\n",
    "    start_col_idx = 0\n",
    "    end_col_idx = num_cols\n",
    "    cols = list(working_df.columns)\n",
    "    total_cols = len(cols)\n",
    "    \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\n\\nrun_Pandas_Profiling Begin\\n')\n",
    "    \n",
    "    if break_into_sections:\n",
    "        while end_col_idx <= total_cols:\n",
    "            #slice the column list by the given num_cols\n",
    "            cols_by_section = cols[start_col_idx:end_col_idx] \n",
    "\n",
    "            #make sure the target variable is in the heat maps\n",
    "            if 'Ever told had hypertension' not in cols_by_section:\n",
    "                cols_by_section.append('Ever told had hypertension')\n",
    "\n",
    "            #create a dataset with only the selected columns\n",
    "            df_to_profile = working_df.loc[:, cols_by_section]\n",
    "\n",
    "            with open(session_log, 'a') as f:   \n",
    "               f.write('\\nProfiling : ' + str(df_to_profile.columns) + '\\n')\n",
    "                \n",
    "            #run the profile\n",
    "            profile = df_to_profile.profile_report(title = 'Pandas Profiling Report')\n",
    "            profile.to_file(output_file = timestamp + '_' + str(start_col_idx) + \"output.html\")\n",
    "\n",
    "            #update the indicies for the next slice or the end slice\n",
    "            start_col_idx = end_col_idx \n",
    "            if (end_col_idx + num_cols > total_cols) and (total_cols % num_cols > 0):\n",
    "                end_col_idx += total_cols % num_cols\n",
    "            else:\n",
    "                end_col_idx += num_cols\n",
    "    else:\n",
    "        #run the profile on all of the columns\n",
    "        profile = working_df.profile_report(title='Pandas Profiling Report')\n",
    "        profile.to_file(output_file= timestamp + '_' + str(start_col_idx) + \"output.html\")\n",
    "\n",
    "    total_time = datetime.now() - t0\n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nrun_Pandas_Profiling End.  Duration: ' + str(total_time) + '\\n')\n",
    "\n",
    "\n",
    "def run_model(classifier, X_train_data, X_test_data, y_train_data, y_test_data, tuning_model='None', hyperparams={}):\n",
    "    \"\"\"fit the algorithm to training data\n",
    "       predict using testing data\n",
    "       print the confusion matrix & classification report\n",
    "       return F1 score for the positive class\n",
    "    \"\"\"\n",
    "    t0 = datetime.now()\n",
    "\n",
    "    #if performing hyperparameter tuning & cross validation, use those estimators to fit\n",
    "    #otherwise use the basic classifier\n",
    "    if tuning_model == 'Grid':\n",
    "        estimator = GridSearchCV(classifier, hyperparams, cv=5, verbose=0)\n",
    "    elif tuning_model == 'Random':\n",
    "        estimator = RandomizedSearchCV(classifier, hyperparams, n_iter=15, cv=5, verbose=0, random_state=42)\n",
    "    else:\n",
    "        estimator = classifier\n",
    "    \n",
    "    # Fit the classifier to the data\n",
    "    model = estimator.fit(X_train_data, y_train_data)\n",
    "\n",
    "    if tuning_model != 'None':\n",
    "        print('Best parameters: ', sorted(model.best_params_.items()))\n",
    "        \n",
    "    # Predict the labels for the training data X\n",
    "    y_pred = model.predict(X_test_data)\n",
    "    \n",
    "    # Print the confusion matrix and classification report\n",
    "    print('Confusion maxtrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification report: \\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    print('Run time: ', datetime.now() - t0)\n",
    "    # Return the F1 score for the positive class to use for ranking models\n",
    "    return sklearn.metrics.f1_score(y_test, y_pred, labels=None, pos_label='2', sample_weight=None)\n",
    "    \n",
    "\n",
    "def tally_results(all_results):\n",
    "    \"\"\" determine if latest results are better than all_results\n",
    "    \"\"\"\n",
    "    latest = list(all_results.values())[-1]\n",
    "    best = max(all_results.values())\n",
    "    if latest >= best:\n",
    "        print('Improvement over prior models!')\n",
    "    else:\n",
    "        print('No improvement over prior models. Best model so far: {}, {:.2%}'.format( \n",
    "          max(model_results, key=model_results.get), best))\n",
    "    \n",
    "    \n",
    "def plot_results(results_dict):\n",
    "    \"\"\" plot bar chart of results \"\"\"\n",
    "\n",
    "    # create a list of tuples sorted by key\n",
    "    lists = sorted(results_dict.items()) \n",
    "    # unpack tuples\n",
    "    x, y = zip(*lists) \n",
    "    plt.barh(x, y)\n",
    "    plt.title('F1 score results')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "\n",
    "def XGBoost_classifer(X_train_data, X_test_data, y_train_data, y_test_data, colnames, session_log):\n",
    "    \"\"\"Use XGBoost to predict target\n",
    "        Params defined from hyperparameter tuning\n",
    "    \"\"\"\n",
    "    print(\"XGBoost_classifer\")\n",
    "\n",
    "    mod = xgb.XGBClassifier(\n",
    "        learning_rate =0.1,   \n",
    "        n_estimators=151, \n",
    "        max_depth=5,\n",
    "        min_child_weight=1,  #default\n",
    "        gamma=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        objective= 'binary:logistic', # try hinge - make predictions of 0 or 1, rather than probabilities\n",
    "        max_delta_step = 1, #tuning for imbalanced dataset where we care about the probability, not AUC\n",
    "        seed=42) \n",
    "\n",
    "    mod.fit(X_train_data, y_train_data)\n",
    "    y_pred = mod.predict(X_test_data)\n",
    "\n",
    "    print('Confusion maxtrix: \\n', confusion_matrix(y_test_data, y_pred))\n",
    "    print('Classification report: \\n', classification_report(y_test_data, y_pred))\n",
    "    \n",
    "    \n",
    "    coefs = list(zip(colnames, mod.feature_importances_))\n",
    "    coefs.sort(key=lambda tup: tup[1])\n",
    "   \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nXGBoost_classifer\\n')\n",
    "        params = json.dumps(mod.get_params())\n",
    "        f.write('\\nParams' + params + '\\n')\n",
    "        f.write('\\nFeature importances' + '\\n')\n",
    "        for col, value in coefs:\n",
    "            f.write(str(value) + ':' + col + '\\n')\n",
    "        f.write(np.array2string(confusion_matrix(y_test_data, y_pred), separator=', '))\n",
    "        f.write('\\n')\n",
    "    report = classification_report(y_test_data, y_pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(session_log, mode='a')\n",
    "    \n",
    "\n",
    "def XGBoost_grid_search_cv(X_train_data, X_test_data, y_train_data, y_test_data, session_log):\n",
    "    \"\"\"\n",
    "    Test various sets of hyperparameters, updating model with best values\n",
    "\n",
    "    #Round 1:\n",
    "    #hyperparameters = {'max_depth':range(3,10,2),'min_child_weight':range(1,6,2)}\n",
    "    #Best params:  {'max_depth': 5, 'min_child_weight': 1}\n",
    "    \"\"\"\n",
    "\n",
    "    #Round 2:\n",
    "    #hyperparameters = {'gamma':[i/10.0 for i in range(0,5)]}\n",
    "    #Best params:  {'gamma': 0.0} #however, better F1 values with gamma = 1\n",
    "\n",
    "    #Round 3:\n",
    "    #hyperparameters = {'subsample':[i/10.0 for i in range(6,10)],'colsample_bytree':[i/10.0 for i in range(6,10)]}\n",
    "    #Best params:  {'colsample_bytree': 0.6, 'subsample': 0.8}\n",
    "\n",
    "    #Round 4: \n",
    "    #hyperparameters = {'reg_alpha':[0, 0.001, 0.005, 0.01, 0.05]}\n",
    "    #Best params:  {'reg_alpha': 0.001}\n",
    "    #This produced no change in the results, so leaving it out\n",
    "\n",
    "    print(\"XGBoost grid search\")\n",
    "    mod = xgb.XGBClassifier(\n",
    "        learning_rate =0.1,   #more conservative and the default\n",
    "        n_estimators=151,     #optimized # based on xgb.cv\n",
    "        max_depth=5,\n",
    "        min_child_weight=1,  #default\n",
    "        gamma=1,\n",
    "        subsample=0.8,\n",
    "        colsample_bytree=0.6,\n",
    "        #objective= 'binary:hinge', #make predictions of 0 or 1, rather than probabilities\n",
    "        nthread=4,\n",
    "        scale_pos_weight=1, #default\n",
    "        max_delta_step = 1, #tuning for imbalanced dataset where we care about the probability, not AUC\n",
    "        seed=27)\n",
    "\n",
    "    \n",
    "    # Create grid search using 5-fold cross validation\n",
    "    print('grid search')\n",
    "    clf = GridSearchCV(mod, hyperparameters, cv=5, verbose=0)\n",
    "\n",
    "    best_model = clf.fit(X_train_data, y_train_data)\n",
    "\n",
    "    # View best hyperparameters\n",
    "    print('Best n_estimator:', best_model.best_estimator_.get_params()['n_estimators'])\n",
    "    print('Best learning rate:', best_model.best_estimator_.get_params()['learning_rate'])\n",
    "    print('Best params: ', best_model.best_params_)\n",
    "    df_results = best_model.cv_results_\n",
    "    print('Results: ', df_results )\n",
    "  \n",
    "    y_pred = best_model.predict(X_test_data)\n",
    "\n",
    "    print('Confusion maxtrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "    print('Classification report: \\n', classification_report(y_test, y_pred))\n",
    "    \n",
    "    \n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nXGBoost_grid_search_sv\\n')\n",
    "        params = json.dumps(best_model.best_params_)\n",
    "        f.write('\\nBest params' + params + '\\n')\n",
    "        #f.write('\\nResults' + best_model.cv_results_ + '\\n')\n",
    "        f.write(np.array2string(confusion_matrix(y_test_data, y_pred), separator=', '))\n",
    "        f.write('\\n')\n",
    "        \n",
    "    report = classification_report(y_test_data, y_pred, output_dict=True)\n",
    "    df = pd.DataFrame(report).transpose()\n",
    "    df.to_csv(session_log, mode='a')\n",
    "\n",
    "\n",
    "def xgb_cv(X_train_data, X_test_data, y_train_data, y_test_data, session_log, cv_folds=5, early_stopping_rounds=50):\n",
    "    #use xgb cross validation to find optimal n_estimators\n",
    "    #trains until cv error hasn't reduced by early_stopping_rounds\n",
    "    \n",
    "    #using DMatrix requires label to be [0, 1], in data 1=no, 2=yes\n",
    "    y_train_data[y_train_data == '1'] = 0\n",
    "    y_train_data[y_train_data == '2'] = 1\n",
    "    y_test_data[y_test_data == '1'] = 0\n",
    "    y_test_data[y_test_data == '2'] = 1\n",
    "\n",
    "    xgb_param = {\n",
    "    'learning_rate': 0.1, \n",
    "    'n_estimators' : 151,\n",
    "    'max_depth':5,\n",
    "    'min_child_weight':1,\n",
    "    'gamma':1,\n",
    "    'subsample': 0.8, \n",
    "    'colsample_bytree': 0.6, \n",
    "    'objective': 'binary:logistic',\n",
    "    'max_delta_step' : 1, #tuning for imbalanced dataset where we care about the probability, not AUC \n",
    "    'seed':27, \n",
    "               } \n",
    "\n",
    "    \n",
    "    #xgb_param = alg.get_xgb_params()\n",
    "    xgtrain = xgb.DMatrix(X_train_data, label=y_train_data)\n",
    "    xgtest = xgb.DMatrix(X_test_data)\n",
    "\n",
    "    #use error as metric for binary classification problem\n",
    "    cvresult = xgb.cv(xgb_param, xgtrain, num_boost_round=xgb_param['n_estimators'], nfold=cv_folds,\n",
    "        metrics=['error'], early_stopping_rounds=early_stopping_rounds)\n",
    "\n",
    "    print('optimal n_estimators from xgb.cv : ', str(cvresult.shape[0]))\n",
    "\n",
    "    #update the n_estimators returned from xgb.cv\n",
    "    xgb_param['n_estimators'] = cvresult.shape[0]\n",
    "    final_gb = xgb.train(xgb_param, xgtrain, num_boost_round = cvresult.shape[0])\n",
    "    final_gb.params = xgb_param\n",
    "    \n",
    "    \n",
    "    #get feature importances\n",
    "    importances = final_gb.get_fscore()\n",
    "\n",
    "    with open(session_log, 'a') as f:   \n",
    "        f.write('\\nXgb_cv\\n')\n",
    "        params = json.dumps(xgb_param)\n",
    "        f.write('\\nParams used in cv' + params + '\\n')\n",
    "        f.write('\\noptimal n_estimators:' + str(cvresult.shape[0]) + '\\n')\n",
    "        fet_imp = json.dumps(importances)\n",
    "        f.write('\\nFeature importances ' + fet_imp + '\\n')\n",
    "\n",
    "    #Fit the algorithm on the data\n",
    "    final_gb.fit(X_train_data, y_train_data, eval_metric='auc')\n",
    "\n",
    "    #Predict on test set\n",
    "    y_pred = final_gb.predict(xgtest) \n",
    "\n",
    "    #the predict function for XGBoost outputs probabilities by default and not actual class labels\n",
    "    y_pred[y_pred > 0.5] = 1\n",
    "    y_pred[y_pred <= 0.5] = 0\n",
    "    \n",
    "    print('\\naccuracy_score  ', str(accuracy_score(y_pred, y_test)))\n",
    "    \n",
    "    cm = confusion_matrix(y_test_data, (y_pred>0.5))\n",
    "    print(cm)\n",
    "\n",
    "    # dump model with feature map\n",
    "    final_gb.dump_model('dump.raw.txt', 'featmap.txt')   \n",
    "\n",
    "    #print('Confusion maxtrix: \\n', confusion_matrix(y_test, y_pred))\n",
    "    #print('Classification report: \\n', classification_report(y_test, y_pred))\n",
    "\n",
    "    #print(\"AUC Score (Train): %f\" % metrics.roc_auc_score(y_test_data, y_predprob)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Main Calling Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Raw dataframe shape:  (2061980, 51)\n"
     ]
    }
   ],
   "source": [
    "#initialize run parameters\n",
    "t0 = datetime.now()\n",
    "working_file = 'nhis_00010.csv'\n",
    "col_info_file = 'Extract9ColMapping.csv'\n",
    "cols_to_drop_file = 'Extract9ColsToDrop.csv'\n",
    "timestamp = t0.strftime(\"%Y%m%d-%H%M%S\")\n",
    "log_file = 'results' + timestamp + '.txt'\n",
    "final_file = 'finaldf_' + timestamp + '.csv'\n",
    "model_results = {}\n",
    "\n",
    "#read the raw dataset, using the survey year as the index and setting all fields to categorical datatype\n",
    "df = pd.read_csv(working_file, dtype='category', index_col = 'YEAR') #, nrows=5000)\n",
    "print('Raw dataframe shape: ', df.shape)\n",
    "\n",
    "#record library versions and original columns in raw dataset\n",
    "write_log_header(df, working_file, log_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Data Wrangling to prepare data for use with models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1    120967\n",
      "2     78602\n",
      "8         0\n",
      "9         0\n",
      "7         0\n",
      "0         0\n",
      "Name: Ever told had hypertension, dtype: int64\n",
      "Dataframe shape after wrangling:  (199569, 38)\n"
     ]
    }
   ],
   "source": [
    "#remove all but positive or negative responses for the target variable\n",
    "df = drop_invalid_target_rows(df, log_file)\n",
    "\n",
    "#remove columns with high percentage of unsuable data\n",
    "df = drop_columns(df, cols_to_drop_file, log_file)\n",
    "\n",
    "#rename columns with reable names\n",
    "df = update_columns(df, col_info_file, log_file)\n",
    "cols = df.drop('Ever told had hypertension', axis=1).columns\n",
    "\n",
    "#remove rows wtih unusable data\n",
    "df = drop_missing_data_rows(df, log_file, True)\n",
    "\n",
    "#save dataset used for analysis\n",
    "df.to_csv(final_file)\n",
    "\n",
    "print('Dataframe shape after wrangling: ', df.shape)\n",
    "\n",
    "#Create numpy arrays for the features and the response variable\n",
    "y = df['Ever told had hypertension'].to_numpy()\n",
    "X = df.drop('Ever told had hypertension', axis=1).values\n",
    "\n",
    "#Split into training and test set\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.2, random_state=42)\n",
    "\n",
    "#Scale the data\n",
    "feature_scaler = StandardScaler()\n",
    "X_train = feature_scaler.fit_transform(X_train)\n",
    "X_test = feature_scaler.transform(X_test)\n",
    "\n",
    "#create 0/1 arrays for binary classification\n",
    "y_train_bool = np.array([0 if x == '1' else 1 for x in y_train])\n",
    "y_test_bool = np.array([0 if x == '1' else 1 for x in y_test])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Run ML models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. K Nearest Neighbors\n",
    "\n",
    "This alogrithm predicts the class of a new data point by majority vote of the \"K nearest neighbors\".\n",
    "\n",
    "Pros: The algorithm is simple and it is easy to visualize the methodology.\n",
    "\n",
    "Cons: Testing is slow because each test data point needs to be compared to all of the training data points to determine which are closest. This could make it impractical to use for real-time classification.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### KNN with default parameters and rule-of-thumb for K\n",
    "\n",
    "K determines how many neighbors are evaluated. With K at the extreme value of 1, we create a rough decision boundary and \"islands\" of data points, overfitting the model. We would get a new decision boundary for each new dataset, making the variance (how much the algoirthm changes given new data) very high.  The bias is very low, because on average the classification performance is accurate.\n",
    "\n",
    "To reduce the roughness of the decision boundary, we increase K and the algorithm uses a majority vote for classification. This reduces the variance, but increases the bias.  If K is too large, we may make the boundary too smooth and make too many mistakes.\n",
    "\n",
    "The rule-of-thumb for K is to use the square root of the number of features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[21943  2274]\n",
      " [10104  5593]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.68      0.91      0.78     24217\n",
      "           2       0.71      0.36      0.47     15697\n",
      "\n",
      "    accuracy                           0.69     39914\n",
      "   macro avg       0.70      0.63      0.63     39914\n",
      "weighted avg       0.70      0.69      0.66     39914\n",
      "\n",
      "Run time:  0:10:56.811190\n"
     ]
    }
   ],
   "source": [
    "k = int(np.sqrt(len(X_train)))\n",
    "knn = KNeighborsClassifier(n_neighbors=k, weights='uniform', algorithm='auto', leaf_size=30, p=2, \n",
    "                           metric='minkowski', metric_params=None, n_jobs=None)\n",
    "\n",
    "model_results['KNN default'] = run_model(knn, X_train, X_test, y_train, y_test, tuning_model='None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Logistic Regression\n",
    "\n",
    "Linear regression predicts the class of a new data point by defining it as a linear function of the features, using coefficients on the features and minimizing the error term ordinary least squares, which represents the distance between the actual data point and the predicted data point. This model is used for predicting qualitative data.\n",
    "\n",
    "For the quantitative data in our dataset, logistic regression is the linear model for classification, which models the probabilities of classification outcomes with a logistic function. The error term to minimize is the mean squared error.  \n",
    "\n",
    "Pros: The concept of the algorithm is easy to grasp.\n",
    "\n",
    "Cons: Many assumptions of the data need to be satisfied in order to use this model, primarily that a linear relationship exists between the features and target. This can be difficult to verify. In addition, the complexity of the model increases as the number of features increases, making it difficult to visualize.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[21104  3113]\n",
      " [ 8766  6931]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.87      0.78     24217\n",
      "           2       0.69      0.44      0.54     15697\n",
      "\n",
      "    accuracy                           0.70     39914\n",
      "   macro avg       0.70      0.66      0.66     39914\n",
      "weighted avg       0.70      0.70      0.69     39914\n",
      "\n",
      "Run time:  0:00:03.442267\n"
     ]
    }
   ],
   "source": [
    "log_reg = LogisticRegression(solver='lbfgs', max_iter=4000)\n",
    "model_results['LogReg default'] = run_model(log_reg, X_train, X_test, y_train, y_test, tuning_model='None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results show an improvement over the f1-scores for negative and positive classifications from the K-NN model. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Random Forest\n",
    "This algorithm uses an ensemble of decision trees. The individual trees are formed by branching on each of the features in the dataset. Each leaf is a possible outcome for the feature.  Creating an ensemble avoids the problem of overfitting that commonly occurs in individual trees.\n",
    "\n",
    "Pros: The concept of the algorithm is easy to grasp.\n",
    "\n",
    "Cons: Branching occurs iteratively with an algorithm to maximize information gain and minimize entropy. Without familiarity with information science, it is not intuitive or transparent as to how the path to each leaf is contructed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[19707  4510]\n",
      " [ 7198  8499]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.81      0.77     24217\n",
      "           2       0.65      0.54      0.59     15697\n",
      "\n",
      "    accuracy                           0.71     39914\n",
      "   macro avg       0.69      0.68      0.68     39914\n",
      "weighted avg       0.70      0.71      0.70     39914\n",
      "\n",
      "Run time:  0:00:58.950242\n"
     ]
    }
   ],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=100, random_state=42)\n",
    "model_results['Random Forest default'] = run_model(rfc, X_train, X_test, y_train, y_test, tuning_model='None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. AdaBoost\n",
    "The Adaptive Boosting algorithm uses repeated training of weak learners on weighted training data, where incorrect predictions are given more influence, so subsequent learners are directed to focus on examples missed by previous iterations.\n",
    "\n",
    "Pros: The concept of the algorithm is easy to grasp, and boosting algorithms typically produce better results than other algorithms.\n",
    "\n",
    "Cons: The algorithm is more complex than other models and it is difficult to visualize the process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[20808  3409]\n",
      " [ 8027  7670]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.86      0.78     24217\n",
      "           2       0.69      0.49      0.57     15697\n",
      "\n",
      "    accuracy                           0.71     39914\n",
      "   macro avg       0.71      0.67      0.68     39914\n",
      "weighted avg       0.71      0.71      0.70     39914\n",
      "\n",
      "Run time:  0:00:31.383392\n"
     ]
    }
   ],
   "source": [
    "abc = AdaBoostClassifier(n_estimators=50, base_estimator=None, learning_rate=1, random_state=42)\n",
    "\n",
    "model_results['AdaBoost default'] = run_model(abc, X_train, X_test, y_train, y_test, tuning_model='None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Logistic Regression with hyperparameter tuning and cross validation\n",
    "\n",
    "Regularization adds a term to the mean squared error formula, penalizing large coefficients, helping to prevent overfitting. It is applied by default, but can be customized with parameters.\n",
    "\n",
    "C controls the inverse of the regularization strength. A large C can lead to an overfit model, while a small C can lead to an underfit model.\n",
    "\n",
    "L1 minimizes the sum of the absolute differences (S) between the target value (Yi) and the estimated values (f(xi)), aka \"least absolute deviations\".  L2 minimizing the sum of the square of the differences (S) between the target value (Yi) and the estimated values (f(xi), aka \"least squares error\".\n",
    "\n",
    "The liblinear solver is good for small datasets.\n",
    "\n",
    "The balanced mode of the class_weight parameter uses the values of Y to automatically adjust weights inversely proportional to class frequencies in the input data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "logreg = LogisticRegression(max_iter=4000)\n",
    "\n",
    "# Create regularization penalty space\n",
    "penalty_list = ['l1', 'l2']\n",
    "c_space = np.logspace(-5, 10, 15)\n",
    "\n",
    "# Create solver list \n",
    "solver_list = ['liblinear',  'saga']\n",
    "    \n",
    "# try balancing / not balancing\n",
    "class_weight_list = ['balanced', None]\n",
    "\n",
    "hyper_logreg = dict(penalty=penalty_list, C=c_space, solver = solver_list,\n",
    "        class_weight=class_weight_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. Logistic Regression with grid search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('C', 0.0013894954943731374), ('class_weight', None), ('penalty', 'l2'), ('solver', 'liblinear')]\n",
      "Confusion maxtrix: \n",
      " [[21084  3133]\n",
      " [ 8742  6955]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.87      0.78     24217\n",
      "           2       0.69      0.44      0.54     15697\n",
      "\n",
      "    accuracy                           0.70     39914\n",
      "   macro avg       0.70      0.66      0.66     39914\n",
      "weighted avg       0.70      0.70      0.69     39914\n",
      "\n",
      "Run time:  0:15:53.195898\n"
     ]
    }
   ],
   "source": [
    "model_results['LogReg GridSearch'] = run_model(logreg, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_logreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. Logistic Regression with random search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('C', 0.19306977288832497), ('class_weight', None), ('penalty', 'l1'), ('solver', 'liblinear')]\n",
      "Confusion maxtrix: \n",
      " [[21105  3112]\n",
      " [ 8765  6932]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.71      0.87      0.78     24217\n",
      "           2       0.69      0.44      0.54     15697\n",
      "\n",
      "    accuracy                           0.70     39914\n",
      "   macro avg       0.70      0.66      0.66     39914\n",
      "weighted avg       0.70      0.70      0.69     39914\n",
      "\n",
      "Run time:  0:01:56.663322\n"
     ]
    }
   ],
   "source": [
    "model_results['LogReg RandomSearch'] = run_model(logreg, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_logreg)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### AdaBoost with hyperparameter tuning and cross validation\n",
    "\n",
    "n_estimators is the maximum number of weak learners to create.  The default type of weak learner is DecisionTreeClassifier.\n",
    "\n",
    "Learning rate shrinks the contribution of each classifier.\n",
    "\n",
    "The algorithm list allows for calculating class probabilities or discrete values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "abc = AdaBoostClassifier(random_state=42)\n",
    "\n",
    "n_estimator_list = [50, 100, 150]\n",
    "learning_rate_list = [1, 0.5, 0.25]\n",
    "algorithm_list = ['SAMME.R', 'SAMME']  \n",
    "hyper_abc = dict(n_estimators=n_estimator_list, learning_rate=learning_rate_list, algorithm = algorithm_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### i. AdaBoost with grid search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('algorithm', 'SAMME.R'), ('learning_rate', 1), ('n_estimators', 150)]\n",
      "Confusion maxtrix: \n",
      " [[20814  3403]\n",
      " [ 7968  7729]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.86      0.79     24217\n",
      "           2       0.69      0.49      0.58     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.68      0.68     39914\n",
      "weighted avg       0.71      0.72      0.70     39914\n",
      "\n",
      "Run time:  3:08:25.965164\n"
     ]
    }
   ],
   "source": [
    "model_results['AdaBoost GridSearch'] = run_model(abc, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_abc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ii. AdaBoost with random search hyperparameter tuning and cross validation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('algorithm', 'SAMME.R'), ('learning_rate', 1), ('n_estimators', 150)]\n",
      "Confusion maxtrix: \n",
      " [[20814  3403]\n",
      " [ 7968  7729]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.72      0.86      0.79     24217\n",
      "           2       0.69      0.49      0.58     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.68      0.68     39914\n",
      "weighted avg       0.71      0.72      0.70     39914\n",
      "\n",
      "Run time:  2:04:10.026901\n"
     ]
    }
   ],
   "source": [
    "model_results['AdaBoost RandomSearch'] = run_model(abc, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Random', hyperparams = hyper_abc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Random Forest with hyperparameter tuning and cross validation\n",
    "\n",
    "n_estimators is the number of trees in the forest. The larger the better, but this increases the computational cost, and improvements will plateau.\n",
    "\n",
    "max_features is the size of the random subsets of features to consider when splitting a node. Low numbers reduce variance but increase bias. The rule of thumb for classification problems is to use the square root of the number of features. \n",
    "\n",
    "Bootstrap samples are used by default, setting this parameter to false will use the entire dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "n_estimator_list = [100, 150, 200]\n",
    "max_features_size = ['sqrt']\n",
    "bootstrap_list = [True, False]\n",
    "\n",
    "hyper_rfc = dict(n_estimators=n_estimator_list, max_features=max_features_size, bootstrap=bootstrap_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('bootstrap', True), ('max_features', 'sqrt'), ('n_estimators', 200)]\n",
      "Confusion maxtrix: \n",
      " [[19686  4531]\n",
      " [ 7143  8554]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.81      0.77     24217\n",
      "           2       0.65      0.54      0.59     15697\n",
      "\n",
      "    accuracy                           0.71     39914\n",
      "   macro avg       0.69      0.68      0.68     39914\n",
      "weighted avg       0.70      0.71      0.70     39914\n",
      "\n",
      "Run time:  0:31:56.616577\n"
     ]
    }
   ],
   "source": [
    "model_results['Random Forest GridSearch'] = run_model(rfc, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the best value for the number of estimators was at the top of the range given, we can try again with values greater than 200, and used the optimized values for the other parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "n_estimator_list = [200, 400, 600]\n",
    "max_features_size = ['sqrt']\n",
    "bootstrap_list = [True]\n",
    "\n",
    "hyper_rfc = dict(n_estimators=n_estimator_list, max_features=max_features_size, bootstrap=bootstrap_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('bootstrap', True), ('max_features', 'sqrt'), ('n_estimators', 600)]\n",
      "Confusion maxtrix: \n",
      " [[19635  4582]\n",
      " [ 7055  8642]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.81      0.77     24217\n",
      "           2       0.65      0.55      0.60     15697\n",
      "\n",
      "    accuracy                           0.71     39914\n",
      "   macro avg       0.69      0.68      0.68     39914\n",
      "weighted avg       0.70      0.71      0.70     39914\n",
      "\n",
      "Run time:  0:57:29.035663\n"
     ]
    }
   ],
   "source": [
    "model_results['Random Forest GridSearch2'] = run_model(rfc, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the improvement in the F1 score at the top range of the number of estimators, we can repeat the process of finding the optimal number of trees. Of note, tuning n_estimators up to 600 took twice as long as the first tuning round."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(random_state=42)\n",
    "\n",
    "n_estimator_list = [800, 1000, 1500]\n",
    "max_features_size = ['sqrt']\n",
    "bootstrap_list = [True]\n",
    "\n",
    "hyper_rfc = dict(n_estimators=n_estimator_list, max_features=max_features_size, bootstrap=bootstrap_list)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('bootstrap', True), ('max_features', 'sqrt'), ('n_estimators', 1000)]\n",
      "Confusion maxtrix: \n",
      " [[19655  4562]\n",
      " [ 7057  8640]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.81      0.77     24217\n",
      "           2       0.65      0.55      0.60     15697\n",
      "\n",
      "    accuracy                           0.71     39914\n",
      "   macro avg       0.70      0.68      0.68     39914\n",
      "weighted avg       0.70      0.71      0.70     39914\n",
      "\n",
      "Run time:  2:08:22.776582\n"
     ]
    }
   ],
   "source": [
    "model_results['Random Forest GridSearch3'] = run_model(rfc, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_rfc)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This iteration shows that the optimal estimators were not any better at the top end of our range, so we will make this the final round."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. XGBoost\n",
    "The Extreme Gradient Boosting algorithm is an implementation of gradient boosted decision trees.  Like Adaptive Boosting, it trains many models sequentially. Each new model gradually minimizes a loss function of the whole system using the Gradient Descent method, an iterative optimization algorithm for finding the minimum of a function. The goal is to create new base learners that are maximally correlated with a negative gradient of the loss function associated with the whole ensemble.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### a) XGBoost with default parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion maxtrix: \n",
      " [[20594  3623]\n",
      " [ 7722  7975]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.85      0.78     24217\n",
      "           2       0.69      0.51      0.58     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.68      0.68     39914\n",
      "weighted avg       0.71      0.72      0.71     39914\n",
      "\n",
      "Run time:  0:00:45.358605\n"
     ]
    }
   ],
   "source": [
    "#max_delta_step = 1 helps with tuning for imbalanced dataset where we care about the probability, not AUC \n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42)\n",
    "\n",
    "model_results['XGBoost default'] = run_model(xgb_def, X_train, X_test, y_train, y_test, tuning_model='None')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### b) XGBoost with hyperparameter tuning and cross validation\n",
    "XGBoost exposes many parameters that can be tuned. An subset most appliccable to this dataset has been chosen as follows:\n",
    "\n",
    "n_estimators is the maximum number of weak learners to create.  The default type of weak learner is gbtree, a tree based model.\n",
    "\n",
    "Learning rate shrinks the contribution of each classifier.\n",
    "\n",
    "Controlling overfitting:\n",
    "\n",
    "1) Control model complexity with \n",
    "* max_depth - maximum depth of the tree. Inceasing this value increases complexity and risks overfitting.\n",
    "* min_child_weight - if a partition step results in a leaf node with the sum of instance weight less than this parameter, partitioning stops. Higher values reduce complexity.\n",
    "* gamma - minimum loss reduction required to make a further partition. Higher gamma reduces complexity.\n",
    "* reg_alpha - L1 regularization term on weights. Higher values reduce complexity.\n",
    "\n",
    "2) Add randomness to make training resistant to effect of noise in the data with \n",
    "* subsample - the proportion of training data to randomly sample prior to growing trees to prevent overfitting.\n",
    "* colsample_bytree - the subsample ratio of columns to use when constructing each tree, to prevent overfitting.\n",
    "\n",
    "Because of the large number of hyperparameters available, parameter tuning will be done in batches, using tuned parameters in subsequent iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_estimator_list = [800, 1000, 1500]\n",
    "learning_rate_list = [0.01, 0.1, 1]\n",
    "max_depth_list = [3, 5, 7]\n",
    "min_child_weight_list = [0, 2, 4]\n",
    "gamma_list = [0, 1]\n",
    "subsample_list = [i/10.0 for i in range(6,10)]\n",
    "colsample_bytree_list = [i/10.0 for i in range(3,5)]\n",
    "reg_alpha_list = [0, 0.001, 0.005, 0.01]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('learning_rate', 0.1), ('n_estimators', 800)]\n",
      "Confusion maxtrix: \n",
      " [[20263  3954]\n",
      " [ 7290  8407]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.84      0.78     24217\n",
      "           2       0.68      0.54      0.60     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.69      0.69     39914\n",
      "weighted avg       0.71      0.72      0.71     39914\n",
      "\n",
      "Run time:  9:43:20.989172\n"
     ]
    }
   ],
   "source": [
    "#Round 1:\n",
    "hyper_xgb = dict(n_estimators=n_estimator_list, learning_rate=learning_rate_list)\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42)\n",
    "model_results['XGBoost GridSearch'] = run_model(xgb_def, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_xgb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The learning rate and number of estimators will be set to the values of 0.1 and 800 tuned in round 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('max_depth', 3), ('min_child_weight', 4)]\n",
      "Confusion maxtrix: \n",
      " [[20262  3955]\n",
      " [ 7313  8384]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.73      0.84      0.78     24217\n",
      "           2       0.68      0.53      0.60     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.69      0.69     39914\n",
      "weighted avg       0.71      0.72      0.71     39914\n",
      "\n",
      "Run time:  14:33:34.090771\n"
     ]
    }
   ],
   "source": [
    "#Round 2:\n",
    "hyper_xgb = dict(max_depth = max_depth_list, min_child_weight = min_child_weight_list)\n",
    "#Use tuned parameters from round 1\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42, \n",
    "                            n_estimators=800, learning_rate=0.1) \n",
    "model_results['XGBoost GridSearch2'] = run_model(xgb_def, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('gamma', 1), ('subsample', 0.9)]\n",
      "Confusion maxtrix: \n",
      " [[20216  4001]\n",
      " [ 7268  8429]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.83      0.78     24217\n",
      "           2       0.68      0.54      0.60     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.69      0.69     39914\n",
      "weighted avg       0.71      0.72      0.71     39914\n",
      "\n",
      "Run time:  5:16:19.582319\n"
     ]
    }
   ],
   "source": [
    "#Round 3:\n",
    "hyper_xgb = dict(gamma = gamma_list, subsample = subsample_list)\n",
    "#Use tuned parameters from round 2\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42,\n",
    "                            n_estimators=800, learning_rate=0.1,\n",
    "                            max_depth=3, min_child_weight=4)\n",
    "model_results['XGBoost GridSearch3'] = run_model(xgb_def, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best parameters:  [('colsample_bytree', 0.4), ('reg_alpha', 0.01)]\n",
      "Confusion maxtrix: \n",
      " [[20283  3934]\n",
      " [ 7256  8441]]\n",
      "Classification report: \n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "           1       0.74      0.84      0.78     24217\n",
      "           2       0.68      0.54      0.60     15697\n",
      "\n",
      "    accuracy                           0.72     39914\n",
      "   macro avg       0.71      0.69      0.69     39914\n",
      "weighted avg       0.72      0.72      0.71     39914\n",
      "\n",
      "Run time:  2:49:45.539969\n"
     ]
    }
   ],
   "source": [
    "#Round 4:\n",
    "hyper_xgb = dict(colsample_bytree = colsample_bytree_list, reg_alpha = reg_alpha_list)\n",
    "#Use tuned parameters from round 3\n",
    "xgb_def = xgb.XGBClassifier(max_delta_step = 1, seed=42, \n",
    "                            n_estimators=800, learning_rate=0.1,\n",
    "                            max_depth=3, min_child_weight=4,\n",
    "                            gamma=1, subsample=0.9)\n",
    "model_results['XGBoost GridSearch4'] = run_model(xgb_def, X_train, X_test, y_train, y_test,\n",
    "                                               tuning_model = 'Grid', hyperparams = hyper_xgb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_results(model_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
